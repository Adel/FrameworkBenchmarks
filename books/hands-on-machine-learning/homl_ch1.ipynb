{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnxVgpmg2bwr0ciVs/yfOG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adel/FrameworkBenchmarks/blob/master/books/hands-on-machine-learning/homl_ch1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1\n",
        "\n",
        "## What is machine learning\n",
        "\n",
        "> Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed. ( Arthur Samuel, 1959)\n",
        "\n",
        "> A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. (Tom Mitchell, 1997)\n",
        "\n",
        "* Training set: Examples that the system uses to learn. Each sample is called training instance\n",
        "* The part that learns and makes predictions is called the model.\n",
        "* _Accuracy_ is the ratio of correctly classified instances to the total number of instances in classification tasks.\n",
        "\n",
        "## Machine learning use cases\n",
        "### Spam filter\n",
        "\n",
        "#### Traditional approach\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S0XFiqMKT9sMFHqzfNfXZg.png\"/>\n",
        "\n",
        "#### ML approach\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1348/format:webp/1*Zw8YfFSQS3LKurbkciHSdw.png\"/>\n",
        "\n",
        "\n",
        "### Machine learning can help humans learn\n",
        "<img src=\"https://hkalabs.com/wp-content/uploads/2022/11/06_machineLearningCanHelpHumansLearn.png\"/>\n",
        "\n",
        "### Example applications\n",
        "* Analyzing images of products in a production line to automatically classify them\n",
        "* Detecting tumors in brain scans\n",
        "* Automatically classifying news articles\n",
        "* Automatically flagging offensive comments on discussion forums\n",
        "* Summarizing long documents automaticallly\n",
        "* Creating a chatbot or a personall assistant\n",
        "* etc.\n",
        "\n",
        "## Types of machine learning systems\n",
        "\n",
        "### Training supervision\n",
        "\n",
        "#### Supervised learning\n",
        "* The training set contains the desired solutions, called labels.\n",
        "##### Examples\n",
        "* Spam classification\n",
        "* Predicting a target numeric value such as the price of a house\n",
        "\n",
        "#### Unsupervised learning\n",
        "* The training data is unlabeled. The system tries to learn without a teacher.\n",
        "\n",
        "##### Examples\n",
        "* A clustering algorithm\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/merge3cluster.jpg\"/>\n",
        "* Visualization algorithms\n",
        "  * Output a 2D or 3D representation of the data\n",
        "  * Help understand how the data is organized\n",
        "  * Related task: Dimensionality reduction which simplifies the data without loosing too much information. For example, by merging features into one, as a car's mileage and age, which are usually corrolated. This is called feature extraction.\n",
        "  * Anomaly detection, for example, detecting unsual credit card transactions to prevent fraud, catching manufacturing defects, or automatically removing outliers from a dataset before feeding it to another learning algorithm.\n",
        "  * A very similar task is novelty detection\n",
        "* Association rule learning\n",
        "  * Example: Running an association rule on sales logs for a supermarket might reveal that people who purchase barbecure sauce and potato chips also tend to buy steak. Thus, you may want to place those items close to each other.\n",
        "\n",
        "#### Semi-supervised learning\n",
        "For example, photo-hosting services, such as Google photos. It automatically recognizes thhat the same person shows in multiple photos (clustering). Adding the person's name to a photo will allow it to identify it in all photos, which is useful for searching photos.\n",
        "\n",
        "* Most semi-supervised learning algos. are a combination of unsupervised and supervised algorithms.\n",
        "\n",
        "#### Self supervised learning\n",
        "\n",
        "Self-supervised learning (SSL) is a machine learning approach where a model learns representations from unlabeled data by generating its own supervision signals. This is typically done through **pretext tasks** that help the model extract meaningful features.\n",
        "\n",
        "##### Key Approaches:\n",
        "- **Contrastive Learning**: Distinguishing similar and dissimilar data points (e.g., SimCLR, MoCo).\n",
        "- **Predictive Learning**: Predicting missing or transformed parts of the data (e.g., BERT, wav2vec2).\n",
        "\n",
        "##### Benefits:\n",
        "- Eliminates the need for labeled data.\n",
        "- Improves generalization for downstream tasks.\n",
        "- Leverages large-scale unlabeled datasets efficiently.\n",
        "\n",
        "SSL is widely used in **NLP, computer vision, and speech processing**, making it a powerful tool for representation learning.\n",
        "\n",
        "##### Example\n",
        "With a large dataset of unlabeled images, we can randomly mask a small part of each iage and then train a model to recover the original image. The mask images are used as inputs and the original images are used as the labels.\n",
        "\n",
        "* More often than not, such as model is not the final goal, but is fine-tuned for a slighly different task.\n",
        "\n",
        "Note: Transferring knowledge from one task to another is called _transfer learning_ and it's one of the most important techniques in machine learning today.\n",
        "\n",
        "#### Reinforcement learning\n",
        "The learning system, called an _agent_ in this context, can observe the environment, select and perform actions, and get _rewards_ in return (or _penalties_ in the form of negative rewards).\n",
        "\n",
        "##### Examples\n",
        "* Many robots implement reinforcement learning algorithms to learn how to walk.\n",
        "* AlphaGo program is a good example. It learned its winning policies by analyzing millions of games, and playing many games againt itself. it beat Ke Jie, the number one ranked player in the world at the time, at Go.\n",
        "\n",
        "#### Batch versus Online learning\n",
        "\n",
        "##### Batch learning\n",
        "* The system is incapable of learning incrementally. It must be trained using all the available data. This is generally done offline: the system is first trained then deployed into production. This is called _offline learning_.\n",
        "* The model's performance tends to decay slowly over time, simply because the world continues to evolve while the model remian unchanged. This is called, _model rot_ or _model drift_.\n",
        "* Solution: regularly retrain the model.\n",
        "\n",
        "##### Online learning (incremental learning)\n",
        "* Training the model incrementally by feeding its data sequentially, either individually or in mini-batches.\n",
        "\n",
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781098125967/files/assets/mls3_0114.png\"/>\n",
        "\n",
        "\n",
        "* Can be used to train algorithms on huge datasets that cannot fit in one machine's main memory.\n",
        "\n",
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781098125967/files/assets/mls3_0115.png\"/>\n",
        "\n",
        "###### Learning rate\n",
        "* How fast does the system adapt to changing data.\n",
        "* Slow learning rate: the system will have more inertia, and learn more slowly.\n",
        "* High learning rate: Rapidly adopt new data, but quickly forget old data.\n",
        "\n",
        "##### Challenges\n",
        "* Dealing with bad data, as it can impact an decline the system's performance.\n",
        "* Need to monitor the sysstem and switch learning off if drops in performance is detected.\n",
        "\n",
        "#### Instance-Based vs Model-Based learning\n",
        "* How well does the system generalize.\n",
        "\n",
        "##### Instance based learning\n",
        "* The system learns examples by heart, then generalizes to new cases by using a similarity measure to compare them to the learned example (or a subset of them).\n",
        "\n",
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781098125967/files/assets/mls3_0116.png\"/>\n",
        "\n",
        "##### Model-based learning\n",
        "* Build a model and use that model to make predictions.\n",
        "* Typical machine learning worfklow.\n",
        "\n",
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781098125967/files/assets/mls3_0117.png\"/>\n",
        "\n",
        "## Main challenges of Machine learning\n",
        "\n",
        "### Insufficient quantity of training data\n",
        "* Even for very simple problems, we typically need thousands of examples, and for complext problems such as image or speech recognition, we may need millions of examples (unless we can reuse parts of an existing model).\n",
        "\n",
        "* In 2001, Microsoft [researchers](https://dl.acm.org/doi/10.3115/1073012.1073017) showed that very different machine learning algorithms, including fairly simple ones, performed almost identically well on a complex problem of natural language disambiguation once they were given enough data.\n",
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781098125967/files/assets/mls3_0121.png\"/>\n",
        "* [The unreasonable effectiveness of data](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf)\n",
        "* However, small and medium datasets are still very common, and its not always cheap to get extra training data.\n",
        "\n",
        "### Nonrepresentative training data\n",
        "By using an nonrepresentative training set, a model is unlikely to make accurate predictions, as for example for very poor and rich countries below:\n",
        "\n",
        "\n",
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781098125967/files/assets/mls3_0122.png\"/>\n",
        "\n",
        "* Too small sample will have _sampling noise_.\n",
        "#### Sampling bias\n",
        "* Very large samples can be nonrepresentative if the sampling method is flawed: _sampling bias_.\n",
        "* For example, during the US presidential election in1936,the Literary digest conducted a very large poll, sending mail to about 10 million people. It goes 2.4 million answers, but still predicted the result incorrectly, because of sampling bias:\n",
        "* Most mailing addresses obtained tended to favor wealthier people.\n",
        "* less than 25% of the people where were polled answered. This is a special kind of sampling bias called _nonresponse bias_.\n",
        "\n",
        "### Poor-quality data\n",
        "* Training data full of errors, outliers and noise (e.g, due to poor-quality measurements).\n",
        "\n",
        "For example:\n",
        "* Remove outliers.\n",
        "* If 5% of customers did not specify their age (a features), we can:\n",
        "  * ignore the attribute\n",
        "  * ignore those instances\n",
        "  * fill in the missing vlaues\n",
        "  * train two models, one with and one without.\n",
        "\n",
        "### Irrelevant features\n",
        "* Garbage in, garbage out.\n",
        "* Feature engineering: Coming up with a good set of features to train on.\n",
        "* Feature selection: Seleciton the most useful features.\n",
        "* Feature extraction: Combining existing features to produce a more useful one).\n",
        "* Creating new features by gathering new data\n",
        "\n",
        "### Overfitting the training data\n",
        "\n",
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781098125967/files/assets/mls3_0123.png\"/>\n",
        "\n",
        "* If the sample is too smal, which introduces sampling noise, the model for complex models such as deep neural networks is likely to detect patterns in the noise itself. These patterns will not generalize to new instances.\n",
        "* For example, feeding the country name to the model for the gdp per capita prediction, might detect that all dat with `w` in their name have a life satisfaction score greater than 7.\n",
        "\n",
        "#### Warning\n",
        "overfitting happens when the model is too complex relative to the amount and noisiness of the training data.\n",
        "###### Possible solutions\n",
        "* Simplify the model by selecting one with fewer parameters (e.g, a linear model rather than a high-degree polynomial), by reducing the number of attributes or by constraining the model.\n",
        "* Gather more training data.\n",
        "* Reduce the noise in the training data (e.g, fix data errors and remove outliers).\n",
        "\n",
        "### Underfitting the training data\n",
        "* Underfitting is the opposite of overfitting.\n",
        "* It occurs when the model is too simple to learn the underlying structure of data.\n",
        "\n",
        "#### Main options to fix the problem\n",
        "* Select a more powerful model, with more paramaters.\n",
        "* Feed better features to the learning algorithm (feature engineering).\n",
        "* Reduce the constraints of the model (for example, by reducing the regularization hyperparameter).\n",
        "\n",
        "## Testing and validating\n",
        "* Split the data into two sets.\n",
        "  * _Training set_ and _test_ set.\n",
        "* Train the model on the training set and test it using the test set.\n",
        "* The error rate on new cases is called _generalization error_ (or _out-of-sample_ error.\n",
        "* If the training error is low, but the generalization is high, the model is overfitting the data.\n",
        "* It is common to use 80% of the data for training and 20% for testing. However, it depends on the size of the dataset.\n",
        "\n",
        "## Hyperparameter tuning and model selection\n",
        "TODO\n",
        "\n",
        "## Data mismatch\n",
        "* Both the validation set and the test set must be as representative as possible of the data you expect to use in production.\n",
        "\n",
        "\n",
        "TODO\n",
        "\n",
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781098125967/files/assets/mls3_0126.png\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "5TLZOyonepaA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_RxgGgNNvi9G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}